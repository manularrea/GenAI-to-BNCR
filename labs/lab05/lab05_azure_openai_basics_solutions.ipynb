{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 05: First Steps with Azure OpenAI\n",
    "\n",
    "**Course:** Generative AI for Banking Sector  \n",
    "**Institution:** Banco Nacional de Costa Rica (BNCR)  \n",
    "**Instructor:** Manuela Larrea  \n",
    "**Duration:** 3 hours\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this lab, you will be able to:\n",
    "\n",
    "1. Understand the Azure OpenAI Service architecture and authentication\n",
    "2. Make your first API calls to GPT-3.5 and GPT-4 models\n",
    "3. Explore different parameters (temperature, max_tokens, top_p)\n",
    "4. Build a simple banking assistant chatbot\n",
    "5. Handle errors and implement retry logic\n",
    "6. Understand token usage and cost optimization\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Azure Infrastructure for This Lab\n",
    "\n",
    "```\n",
    "‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n",
    "‚ïë                    LAB 05 - AZURE INFRASTRUCTURE                         ‚ïë\n",
    "‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n",
    "\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ                        YOU (Jupyter Notebook)                            ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                                 ‚îÇ\n",
    "                                 ‚îÇ HTTPS Request\n",
    "                                 ‚îÇ (API Key in Header)\n",
    "                                 ‚ñº\n",
    "                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "                    ‚îÇ   Azure OpenAI Service     ‚îÇ\n",
    "                    ‚îÇ                            ‚îÇ\n",
    "                    ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ\n",
    "                    ‚îÇ  ‚îÇ  GPT-3.5 Turbo       ‚îÇ  ‚îÇ\n",
    "                    ‚îÇ  ‚îÇ  (60K TPM)           ‚îÇ  ‚îÇ\n",
    "                    ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ\n",
    "                    ‚îÇ                            ‚îÇ\n",
    "                    ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ\n",
    "                    ‚îÇ  ‚îÇ  GPT-4               ‚îÇ  ‚îÇ\n",
    "                    ‚îÇ  ‚îÇ  (10K TPM)           ‚îÇ  ‚îÇ\n",
    "                    ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ\n",
    "                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "\n",
    "üìä Resources Used:\n",
    "  ‚Ä¢ Azure OpenAI Service (East US 2)\n",
    "  ‚Ä¢ GPT-3.5 Turbo deployment\n",
    "  ‚Ä¢ GPT-4 deployment (optional)\n",
    "\n",
    "üí∞ Estimated Cost: ~$0.50 per lab session\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Environment Setup\n",
    "\n",
    "First, let's install the required packages and set up our environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install openai python-dotenv -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import AzureOpenAI\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Verify environment variables are loaded\n",
    "print(\"‚úì Environment variables loaded\")\n",
    "print(f\"‚úì Azure OpenAI Endpoint: {os.getenv('AZURE_OPENAI_ENDPOINT')[:30]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Initialize Azure OpenAI Client\n",
    "\n",
    "The Azure OpenAI client requires three key pieces of information:\n",
    "1. **API Key**: Your authentication credential\n",
    "2. **Endpoint**: Your Azure OpenAI resource URL\n",
    "3. **API Version**: The version of the API to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Azure OpenAI client\n",
    "client = AzureOpenAI(\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "    api_version=\"2024-02-15-preview\",\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    ")\n",
    "\n",
    "# Deployment names\n",
    "GPT35_DEPLOYMENT = os.getenv(\"AZURE_OPENAI_DEPLOYMENT_GPT35\", \"gpt-35-turbo\")\n",
    "GPT4_DEPLOYMENT = os.getenv(\"AZURE_OPENAI_DEPLOYMENT_GPT4\", \"gpt-4\")\n",
    "\n",
    "print(\"‚úì Azure OpenAI client initialized successfully\")\n",
    "print(f\"‚úì GPT-3.5 Deployment: {GPT35_DEPLOYMENT}\")\n",
    "print(f\"‚úì GPT-4 Deployment: {GPT4_DEPLOYMENT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Your First API Call\n",
    "\n",
    "Let's make our first call to Azure OpenAI! We'll create a simple banking assistant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple chat completion\n",
    "response = client.chat.completions.create(\n",
    "    model=GPT35_DEPLOYMENT,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"Eres un asistente bancario profesional del BNCR.\"},\n",
    "        {\"role\": \"user\", \"content\": \"¬øQu√© es una cuenta de ahorros?\"}\n",
    "    ],\n",
    "    temperature=0.7,\n",
    "    max_tokens=150\n",
    ")\n",
    "\n",
    "print(\"Assistant Response:\")\n",
    "print(response.choices[0].message.content)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"Tokens used: {response.usage.total_tokens}\")\n",
    "print(f\"Model: {response.model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Understanding Message Roles\n",
    "\n",
    "Azure OpenAI uses three message roles:\n",
    "\n",
    "1. **System**: Sets the behavior and context for the assistant\n",
    "2. **User**: The user's input or question\n",
    "3. **Assistant**: The model's previous responses (for conversation history)\n",
    "\n",
    "Let's see how different system prompts affect the responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different system prompts\n",
    "system_prompts = [\n",
    "    \"You are a formal banking advisor.\",\n",
    "    \"You are a friendly banking assistant who explains things simply.\",\n",
    "    \"You are a technical banking expert who provides detailed explanations.\"\n",
    "]\n",
    "\n",
    "user_question = \"What is compound interest?\"\n",
    "\n",
    "for i, system_prompt in enumerate(system_prompts, 1):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Test {i}: {system_prompt}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=GPT35_DEPLOYMENT,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_question}\n",
    "        ],\n",
    "        temperature=0.7,\n",
    "        max_tokens=150\n",
    "    )\n",
    "    \n",
    "    print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Exploring Temperature Parameter\n",
    "\n",
    "**Temperature** controls the randomness of the model's output:\n",
    "- **0.0**: Deterministic, always picks the most likely token\n",
    "- **0.7**: Balanced creativity and consistency (default)\n",
    "- **1.0+**: More creative and random\n",
    "\n",
    "For banking applications, we typically use lower temperatures (0.3-0.7) for consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different temperatures\n",
    "temperatures = [0.0, 0.5, 1.0, 1.5]\n",
    "question = \"Give me 3 tips for saving money.\"\n",
    "\n",
    "for temp in temperatures:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Temperature: {temp}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=GPT35_DEPLOYMENT,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a financial advisor.\"},\n",
    "            {\"role\": \"user\", \"content\": question}\n",
    "        ],\n",
    "        temperature=temp,\n",
    "        max_tokens=200\n",
    "    )\n",
    "    \n",
    "    print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Building a Conversational Banking Assistant\n",
    "\n",
    "Let's build a simple chatbot that maintains conversation history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BankingAssistant:\n",
    "    def __init__(self, client, deployment_name):\n",
    "        self.client = client\n",
    "        self.deployment_name = deployment_name\n",
    "        self.conversation_history = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"\"\"Eres un asistente bancario profesional del Banco Nacional de Costa Rica (BNCR).\n",
    "                You help customers with:\n",
    "                - Account information\n",
    "                - Product recommendations\n",
    "                - General banking questions\n",
    "                - Transaction support\n",
    "                \n",
    "                Always be professional, friendly, and provide accurate information.\n",
    "                If you don't know something, admit it and suggest contacting customer service.\"\"\"\n",
    "            }\n",
    "        ]\n",
    "    \n",
    "    def chat(self, user_message):\n",
    "        # Add user message to history\n",
    "        self.conversation_history.append({\n",
    "            \"role\": \"user\",\n",
    "            \"content\": user_message\n",
    "        })\n",
    "        \n",
    "        # Get response from Azure OpenAI\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=self.deployment_name,\n",
    "            messages=self.conversation_history,\n",
    "            temperature=0.7,\n",
    "            max_tokens=300\n",
    "        )\n",
    "        \n",
    "        # Extract assistant's response\n",
    "        assistant_message = response.choices[0].message.content\n",
    "        \n",
    "        # Add assistant's response to history\n",
    "        self.conversation_history.append({\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": assistant_message\n",
    "        })\n",
    "        \n",
    "        return assistant_message, response.usage\n",
    "    \n",
    "    def reset(self):\n",
    "        # Keep only the system message\n",
    "        self.conversation_history = [self.conversation_history[0]]\n",
    "        print(\"Conversation history reset.\")\n",
    "\n",
    "# Initialize the assistant\n",
    "assistant = BankingAssistant(client, GPT35_DEPLOYMENT)\n",
    "print(\"‚úì Banking Assistant initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test conversation with context\n",
    "print(\"User: Hello! I want to open a savings account.\\n\")\n",
    "response, usage = assistant.chat(\"Hello! I want to open a savings account.\")\n",
    "print(f\"Assistant: {response}\\n\")\n",
    "print(f\"Tokens used: {usage.total_tokens}\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nUser: ¬øQu√© documentos necesito?\\n\")\n",
    "response, usage = assistant.chat(\"¬øQu√© documentos necesito?\")\n",
    "print(f\"Assistant: {response}\\n\")\n",
    "print(f\"Tokens used: {usage.total_tokens}\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nUser: What's the minimum deposit?\\n\")\n",
    "response, usage = assistant.chat(\"What's the minimum deposit?\")\n",
    "print(f\"Assistant: {response}\\n\")\n",
    "print(f\"Tokens used: {usage.total_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Error Handling and Retry Logic\n",
    "\n",
    "Production applications need robust error handling. Let's implement retry logic for common errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from openai import RateLimitError, APIError, APIConnectionError\n",
    "\n",
    "def chat_with_retry(client, messages, deployment, max_retries=3):\n",
    "    \"\"\"\n",
    "    Make a chat completion request with exponential backoff retry\n",
    "    \"\"\"\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=deployment,\n",
    "                messages=messages,\n",
    "                temperature=0.7,\n",
    "                max_tokens=200\n",
    "            )\n",
    "            return response\n",
    "        \n",
    "        except RateLimitError as e:\n",
    "            if attempt == max_retries - 1:\n",
    "                raise\n",
    "            wait_time = 2 ** attempt\n",
    "            print(f\"Rate limit exceeded. Retrying in {wait_time} seconds...\")\n",
    "            time.sleep(wait_time)\n",
    "        \n",
    "        except (APIError, APIConnectionError) as e:\n",
    "            if attempt == max_retries - 1:\n",
    "                raise\n",
    "            wait_time = 2 ** attempt\n",
    "            print(f\"API error occurred. Retrying in {wait_time} seconds...\")\n",
    "            time.sleep(wait_time)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Unexpected error: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "# Test the retry function\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a banking assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is a credit score?\"}\n",
    "]\n",
    "\n",
    "try:\n",
    "    response = chat_with_retry(client, messages, GPT35_DEPLOYMENT)\n",
    "    print(\"Success!\")\n",
    "    print(response.choices[0].message.content)\n",
    "except Exception as e:\n",
    "    print(f\"Failed after retries: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Token Usage and Cost Optimization\n",
    "\n",
    "Understanding token usage is crucial for cost optimization.\n",
    "\n",
    "**Token Pricing (approximate):**\n",
    "- GPT-3.5 Turbo: $0.0015 per 1K input tokens, $0.002 per 1K output tokens\n",
    "- GPT-4: $0.03 per 1K input tokens, $0.06 per 1K output tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cost(usage, model=\"gpt-35-turbo\"):\n",
    "    \"\"\"\n",
    "    Calculate the cost of an API call\n",
    "    \"\"\"\n",
    "    if \"gpt-4\" in model.lower():\n",
    "        input_cost = (usage.prompt_tokens / 1000) * 0.03\n",
    "        output_cost = (usage.completion_tokens / 1000) * 0.06\n",
    "    else:  # GPT-3.5\n",
    "        input_cost = (usage.prompt_tokens / 1000) * 0.0015\n",
    "        output_cost = (usage.completion_tokens / 1000) * 0.002\n",
    "    \n",
    "    total_cost = input_cost + output_cost\n",
    "    \n",
    "    return {\n",
    "        \"input_tokens\": usage.prompt_tokens,\n",
    "        \"output_tokens\": usage.completion_tokens,\n",
    "        \"total_tokens\": usage.total_tokens,\n",
    "        \"input_cost\": input_cost,\n",
    "        \"output_cost\": output_cost,\n",
    "        \"total_cost\": total_cost\n",
    "    }\n",
    "\n",
    "# Test cost calculation\n",
    "response = client.chat.completions.create(\n",
    "    model=GPT35_DEPLOYMENT,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a banking assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Explain the difference between a checking and savings account.\"}\n",
    "    ],\n",
    "    temperature=0.7,\n",
    "    max_tokens=300\n",
    ")\n",
    "\n",
    "cost_info = calculate_cost(response.usage, response.model)\n",
    "\n",
    "print(\"Response:\")\n",
    "print(response.choices[0].message.content)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\\nCost Analysis:\")\n",
    "print(f\"Input tokens: {cost_info['input_tokens']}\")\n",
    "print(f\"Output tokens: {cost_info['output_tokens']}\")\n",
    "print(f\"Total tokens: {cost_info['total_tokens']}\")\n",
    "print(f\"\\nInput cost: ${cost_info['input_cost']:.6f}\")\n",
    "print(f\"Output cost: ${cost_info['output_cost']:.6f}\")\n",
    "print(f\"Total cost: ${cost_info['total_cost']:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 9: Comparing GPT-3.5 vs GPT-4\n",
    "\n",
    "Let's compare the responses and costs between GPT-3.5 and GPT-4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complex banking question\n",
    "complex_question = \"\"\"A customer has $50,000 to invest. They want low risk but better returns \n",
    "than a savings account. They might need access to the money in 2 years. \n",
    "What would you recommend and why?\"\"\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are an expert financial advisor at BNCR.\"},\n",
    "    {\"role\": \"user\", \"content\": complex_question}\n",
    "]\n",
    "\n",
    "# Test with GPT-3.5\n",
    "print(\"GPT-3.5 Turbo Response:\")\n",
    "print(\"=\"*80)\n",
    "response_35 = client.chat.completions.create(\n",
    "    model=GPT35_DEPLOYMENT,\n",
    "    messages=messages,\n",
    "    temperature=0.7,\n",
    "    max_tokens=400\n",
    ")\n",
    "print(response_35.choices[0].message.content)\n",
    "cost_35 = calculate_cost(response_35.usage, \"gpt-35-turbo\")\n",
    "print(f\"\\nCost: ${cost_35['total_cost']:.6f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# Test with GPT-4 (if available)\n",
    "try:\n",
    "    print(\"GPT-4 Response:\")\n",
    "    print(\"=\"*80)\n",
    "    response_4 = client.chat.completions.create(\n",
    "        model=GPT4_DEPLOYMENT,\n",
    "        messages=messages,\n",
    "        temperature=0.7,\n",
    "        max_tokens=400\n",
    "    )\n",
    "    print(response_4.choices[0].message.content)\n",
    "    cost_4 = calculate_cost(response_4.usage, \"gpt-4\")\n",
    "    print(f\"\\nCost: ${cost_4['total_cost']:.6f}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"\\nCost Comparison:\")\n",
    "    print(f\"GPT-3.5: ${cost_35['total_cost']:.6f}\")\n",
    "    print(f\"GPT-4: ${cost_4['total_cost']:.6f}\")\n",
    "    print(f\"GPT-4 is {cost_4['total_cost']/cost_35['total_cost']:.1f}x more expensive\")\n",
    "except Exception as e:\n",
    "    print(f\"GPT-4 not available: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Practical Exercise 1: Product Recommendation System\n",
    "\n",
    "Create a banking product recommendation system that:\n",
    "1. Asks the customer about their needs\n",
    "2. Recommends appropriate products\n",
    "3. Explains why each product is suitable\n",
    "\n",
    "Use the banking products dataset provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load banking products\n",
    "products_df = pd.read_csv(\"../../datasets/banking/banking_products.csv\")\n",
    "\n",
    "# Display available products\n",
    "print(\"Available Banking Products:\")\n",
    "print(products_df[['product_name', 'product_type', 'interest_rate']].to_string(index=False))\n",
    "\n",
    "# TODO: Create a system prompt that includes product information\n",
    "# TODO: Build a recommendation function\n",
    "# TODO: Test with different customer profiles\n",
    "\n",
    "# Your code here:\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load banking products\n",
    "products_df = pd.read_csv(\"../../datasets/banking/banking_products.csv\")\n",
    "\n",
    "# Display available products\n",
    "print(\"Available Banking Products:\")\n",
    "print(products_df[['product_name', 'product_type', 'interest_rate']].to_string(index=False))\n",
    "\n",
    "# SOLUCI√ìN: Sistema de recomendaci√≥n de productos bancarios\n",
    "def recommend_product(customer_profile, products_df):\n",
    "    \"\"\"\n",
    "    Recomienda productos bancarios basados en el perfil del cliente\n",
    "    \"\"\"\n",
    "    system_message = f\"\"\"Eres un asesor financiero del BNCR especializado en recomendar productos.\n",
    "    \n",
    "Productos disponibles:\n",
    "{products_df[['product_name', 'product_type', 'description', 'interest_rate', 'minimum_balance']].to_string(index=False)}\n",
    "\n",
    "Analiza el perfil del cliente y recomienda los 2-3 productos m√°s adecuados con justificaci√≥n.\"\"\"\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": f\"Mi perfil: {customer_profile}\"}\n",
    "    ]\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=GPT4_DEPLOYMENT,\n",
    "        messages=messages,\n",
    "        temperature=0.7,\n",
    "        max_tokens=300\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Ejemplo de uso\n",
    "customer_profile = \"Tengo 25 a√±os, trabajo como ingeniero, gano ‚Ç°800,000 mensuales, quiero ahorrar para una casa\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RECOMENDACI√ìN PERSONALIZADA\")\n",
    "print(\"=\"*80)\n",
    "recommendation = recommend_product(customer_profile, products_df)\n",
    "print(recommendation)\n",
    "print(\"\\nTokens usados:\", response.usage.total_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a multilingual banking assistant\n",
    "# TODO: Test with Spanish and English queries\n",
    "# TODO: Implement language detection\n",
    "\n",
    "# Your code here:\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SOLUCI√ìN: Asistente bancario multiling√ºe con detecci√≥n de idioma\n",
    "\n",
    "def detect_language(text):\n",
    "    \"\"\"Detecta el idioma del texto (simple heuristic)\"\"\"\n",
    "    spanish_words = ['qu√©', 'c√≥mo', 'cu√°l', 'cuenta', 'pr√©stamo', 'tasa']\n",
    "    english_words = ['what', 'how', 'which', 'account', 'loan', 'rate']\n",
    "    \n",
    "    text_lower = text.lower()\n",
    "    spanish_count = sum(1 for word in spanish_words if word in text_lower)\n",
    "    english_count = sum(1 for word in english_words if word in text_lower)\n",
    "    \n",
    "    return 'es' if spanish_count > english_count else 'en'\n",
    "\n",
    "def multilingual_assistant(user_query):\n",
    "    \"\"\"\n",
    "    Asistente que responde en el idioma de la consulta\n",
    "    \"\"\"\n",
    "    language = detect_language(user_query)\n",
    "    \n",
    "    system_messages = {\n",
    "        'es': \"Eres un asistente bancario profesional del BNCR. Responde en espa√±ol de forma clara y profesional.\",\n",
    "        'en': \"You are a professional banking assistant for BNCR. Respond in English clearly and professionally.\"\n",
    "    }\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_messages[language]},\n",
    "        {\"role\": \"user\", \"content\": user_query}\n",
    "    ]\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=GPT35_DEPLOYMENT,\n",
    "        messages=messages,\n",
    "        temperature=0.7,\n",
    "        max_tokens=150\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'language': language,\n",
    "        'response': response.choices[0].message.content,\n",
    "        'tokens': response.usage.total_tokens\n",
    "    }\n",
    "\n",
    "# Pruebas en ambos idiomas\n",
    "print(\"=\"*80)\n",
    "print(\"ASISTENTE MULTILING√úE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "queries = [\n",
    "    \"¬øCu√°les son las tasas de inter√©s para cuentas de ahorro?\",\n",
    "    \"What are the requirements to open a checking account?\",\n",
    "    \"¬øC√≥mo puedo solicitar un pr√©stamo personal?\",\n",
    "    \"What is the minimum balance for a savings account?\"\n",
    "]\n",
    "\n",
    "for query in queries:\n",
    "    print(f\"\\nConsulta: {query}\")\n",
    "    result = multilingual_assistant(query)\n",
    "    print(f\"Idioma detectado: {result['language']}\")\n",
    "    print(f\"Respuesta: {result['response'][:100]}...\")\n",
    "    print(f\"Tokens: {result['tokens']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement daily budget tracker\n",
    "# TODO: Create cost optimization strategies\n",
    "# TODO: Compare model costs\n",
    "\n",
    "# Your code here:\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SOLUCI√ìN: Rastreador de presupuesto y optimizaci√≥n de costos\n",
    "\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "class TokenBudgetTracker:\n",
    "    def __init__(self, daily_budget_usd=5.0):\n",
    "        self.daily_budget = daily_budget_usd\n",
    "        self.requests = []\n",
    "        \n",
    "        # Precios por modelo (por 1K tokens)\n",
    "        self.pricing = {\n",
    "            'gpt-4o-mini': {'input': 0.00015, 'output': 0.0006},\n",
    "            'gpt-35-turbo': {'input': 0.0005, 'output': 0.0015}\n",
    "        }\n",
    "    \n",
    "    def log_request(self, model, prompt_tokens, completion_tokens):\n",
    "        \"\"\"Registra una solicitud y calcula el costo\"\"\"\n",
    "        model_key = 'gpt-4o-mini' if '4' in model else 'gpt-35-turbo'\n",
    "        \n",
    "        cost = (\n",
    "            (prompt_tokens / 1000) * self.pricing[model_key]['input'] +\n",
    "            (completion_tokens / 1000) * self.pricing[model_key]['output']\n",
    "        )\n",
    "        \n",
    "        self.requests.append({\n",
    "            'timestamp': datetime.now(),\n",
    "            'model': model,\n",
    "            'prompt_tokens': prompt_tokens,\n",
    "            'completion_tokens': completion_tokens,\n",
    "            'total_tokens': prompt_tokens + completion_tokens,\n",
    "            'cost_usd': cost\n",
    "        })\n",
    "        \n",
    "        return cost\n",
    "    \n",
    "    def get_daily_summary(self):\n",
    "        \"\"\"Obtiene resumen del d√≠a actual\"\"\"\n",
    "        today = datetime.now().date()\n",
    "        today_requests = [r for r in self.requests if r['timestamp'].date() == today]\n",
    "        \n",
    "        if not today_requests:\n",
    "            return {'requests': 0, 'total_tokens': 0, 'total_cost': 0, 'budget_remaining': self.daily_budget}\n",
    "        \n",
    "        total_tokens = sum(r['total_tokens'] for r in today_requests)\n",
    "        total_cost = sum(r['cost_usd'] for r in today_requests)\n",
    "        \n",
    "        return {\n",
    "            'requests': len(today_requests),\n",
    "            'total_tokens': total_tokens,\n",
    "            'total_cost': total_cost,\n",
    "            'budget_remaining': self.daily_budget - total_cost,\n",
    "            'budget_used_pct': (total_cost / self.daily_budget) * 100\n",
    "        }\n",
    "    \n",
    "    def recommend_optimization(self):\n",
    "        \"\"\"Recomienda estrategias de optimizaci√≥n\"\"\"\n",
    "        summary = self.get_daily_summary()\n",
    "        \n",
    "        if summary['budget_used_pct'] > 80:\n",
    "            return [\n",
    "                \"‚ö†Ô∏è Has usado m√°s del 80% del presupuesto diario\",\n",
    "                \"‚úì Considera usar GPT-3.5 en lugar de GPT-4 para consultas simples\",\n",
    "                \"‚úì Reduce max_tokens a 100-150 para respuestas m√°s cortas\",\n",
    "                \"‚úì Implementa cach√© para consultas frecuentes\"\n",
    "            ]\n",
    "        elif summary['budget_used_pct'] > 50:\n",
    "            return [\n",
    "                \"‚úì Uso moderado del presupuesto\",\n",
    "                \"‚úì Monitorea consultas complejas que usan muchos tokens\",\n",
    "                \"‚úì Considera batch processing para m√∫ltiples consultas\"\n",
    "            ]\n",
    "        else:\n",
    "            return [\n",
    "                \"‚úÖ Uso eficiente del presupuesto\",\n",
    "                \"‚úì Puedes aumentar max_tokens si necesitas respuestas m√°s detalladas\"\n",
    "            ]\n",
    "\n",
    "# Inicializar tracker\n",
    "tracker = TokenBudgetTracker(daily_budget_usd=5.0)\n",
    "\n",
    "# Simular algunas solicitudes\n",
    "print(\"=\"*80)\n",
    "print(\"RASTREADOR DE PRESUPUESTO Y OPTIMIZACI√ìN\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "test_queries = [\n",
    "    \"¬øQu√© es una cuenta de ahorros?\",\n",
    "    \"Explica los beneficios de un pr√©stamo hipotecario\",\n",
    "    \"¬øCu√°les son las tasas de inter√©s actuales?\"\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    response = client.chat.completions.create(\n",
    "        model=GPT35_DEPLOYMENT,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"Eres un asistente bancario del BNCR.\"},\n",
    "            {\"role\": \"user\", \"content\": query}\n",
    "        ],\n",
    "        temperature=0.7,\n",
    "        max_tokens=100\n",
    "    )\n",
    "    \n",
    "    cost = tracker.log_request(\n",
    "        response.model,\n",
    "        response.usage.prompt_tokens,\n",
    "        response.usage.completion_tokens\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nConsulta: {query}\")\n",
    "    print(f\"Tokens: {response.usage.total_tokens} | Costo: ${cost:.6f}\")\n",
    "\n",
    "# Mostrar resumen\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RESUMEN DEL D√çA\")\n",
    "print(\"=\"*80)\n",
    "summary = tracker.get_daily_summary()\n",
    "print(f\"Solicitudes: {summary['requests']}\")\n",
    "print(f\"Tokens totales: {summary['total_tokens']}\")\n",
    "print(f\"Costo total: ${summary['total_cost']:.4f}\")\n",
    "print(f\"Presupuesto restante: ${summary['budget_remaining']:.4f}\")\n",
    "print(f\"Uso del presupuesto: {summary['budget_used_pct']:.1f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RECOMENDACIONES DE OPTIMIZACI√ìN\")\n",
    "print(\"=\"*80)\n",
    "for rec in tracker.recommend_optimization():\n",
    "    print(rec)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}