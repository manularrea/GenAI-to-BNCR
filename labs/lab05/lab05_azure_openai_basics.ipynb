{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lab 05: First Steps with Azure OpenAI\n",
        "\n",
        "**Course:** Generative AI for Banking Sector  \n",
        "**Institution:** Banco Nacional de Costa Rica (BNCR)  \n",
        "**Instructor:** Manuela Larrea  \n",
        "**Duration:** 3 hours\n",
        "\n",
        "---\n",
        "\n",
        "## Learning Objectives\n",
        "\n",
        "By the end of this lab, you will be able to:\n",
        "\n",
        "1. Understand the Azure OpenAI Service architecture and authentication\n",
        "2. Make your first API calls to GPT-3.5 and GPT-4 models\n",
        "3. Explore different parameters (temperature, max_tokens, top_p)\n",
        "4. Build a simple banking assistant chatbot\n",
        "5. Handle errors and implement retry logic\n",
        "6. Understand token usage and cost optimization\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Azure Infrastructure for This Lab\n",
        "\n",
        "```\n",
        "\u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557\n",
        "\u2551                    LAB 05 - AZURE INFRASTRUCTURE                         \u2551\n",
        "\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d\n",
        "\n",
        "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
        "\u2502                        YOU (Jupyter Notebook)                            \u2502\n",
        "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
        "                                 \u2502\n",
        "                                 \u2502 HTTPS Request\n",
        "                                 \u2502 (API Key in Header)\n",
        "                                 \u25bc\n",
        "                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
        "                    \u2502   Azure OpenAI Service     \u2502\n",
        "                    \u2502                            \u2502\n",
        "                    \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n",
        "                    \u2502  \u2502  GPT-3.5 Turbo       \u2502  \u2502\n",
        "                    \u2502  \u2502  (60K TPM)           \u2502  \u2502\n",
        "                    \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n",
        "                    \u2502                            \u2502\n",
        "                    \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n",
        "                    \u2502  \u2502  GPT-4               \u2502  \u2502\n",
        "                    \u2502  \u2502  (10K TPM)           \u2502  \u2502\n",
        "                    \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n",
        "                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
        "\n",
        "\ud83d\udcca Resources Used:\n",
        "  \u2022 Azure OpenAI Service (East US 2)\n",
        "  \u2022 GPT-3.5 Turbo deployment\n",
        "  \u2022 GPT-4 deployment (optional)\n",
        "\n",
        "\ud83d\udcb0 Estimated Cost: ~$0.50 per lab session\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 1: Environment Setup\n",
        "\n",
        "First, let's install the required packages and set up our environment variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install openai python-dotenv -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from openai import AzureOpenAI\n",
        "from dotenv import load_dotenv\n",
        "import json\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "\n",
        "# Verify environment variables are loaded\n",
        "print(\"\u2713 Environment variables loaded\")\n",
        "print(f\"\u2713 Azure OpenAI Endpoint: {os.getenv('AZURE_OPENAI_ENDPOINT')[:30]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 2: Initialize Azure OpenAI Client\n",
        "\n",
        "The Azure OpenAI client requires three key pieces of information:\n",
        "1. **API Key**: Your authentication credential\n",
        "2. **Endpoint**: Your Azure OpenAI resource URL\n",
        "3. **API Version**: The version of the API to use"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize Azure OpenAI client\n",
        "client = AzureOpenAI(\n",
        "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
        "    api_version=\"2024-02-15-preview\",\n",
        "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
        ")\n",
        "\n",
        "# Deployment names\n",
        "GPT35_DEPLOYMENT = os.getenv(\"AZURE_OPENAI_DEPLOYMENT_GPT35\", \"gpt-35-turbo\")\n",
        "GPT4_DEPLOYMENT = os.getenv(\"AZURE_OPENAI_DEPLOYMENT_GPT4\", \"gpt-4\")\n",
        "\n",
        "print(\"\u2713 Azure OpenAI client initialized successfully\")\n",
        "print(f\"\u2713 GPT-3.5 Deployment: {GPT35_DEPLOYMENT}\")\n",
        "print(f\"\u2713 GPT-4 Deployment: {GPT4_DEPLOYMENT}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 3: Your First API Call\n",
        "\n",
        "Let's make our first call to Azure OpenAI! We'll create a simple banking assistant."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simple chat completion\n",
        "response = client.chat.completions.create(\n",
        "    model=GPT35_DEPLOYMENT,\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful banking assistant for BNCR.\"},\n",
        "        {\"role\": \"user\", \"content\": \"What is a savings account?\"}\n",
        "    ],\n",
        "    temperature=0.7,\n",
        "    max_tokens=150\n",
        ")\n",
        "\n",
        "print(\"Assistant Response:\")\n",
        "print(response.choices[0].message.content)\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(f\"Tokens used: {response.usage.total_tokens}\")\n",
        "print(f\"Model: {response.model}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 4: Understanding Message Roles\n",
        "\n",
        "Azure OpenAI uses three message roles:\n",
        "\n",
        "1. **System**: Sets the behavior and context for the assistant\n",
        "2. **User**: The user's input or question\n",
        "3. **Assistant**: The model's previous responses (for conversation history)\n",
        "\n",
        "Let's see how different system prompts affect the responses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test different system prompts\n",
        "system_prompts = [\n",
        "    \"You are a formal banking advisor.\",\n",
        "    \"You are a friendly banking assistant who explains things simply.\",\n",
        "    \"You are a technical banking expert who provides detailed explanations.\"\n",
        "]\n",
        "\n",
        "user_question = \"What is compound interest?\"\n",
        "\n",
        "for i, system_prompt in enumerate(system_prompts, 1):\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"Test {i}: {system_prompt}\")\n",
        "    print(f\"{'='*80}\\n\")\n",
        "    \n",
        "    response = client.chat.completions.create(\n",
        "        model=GPT35_DEPLOYMENT,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": user_question}\n",
        "        ],\n",
        "        temperature=0.7,\n",
        "        max_tokens=150\n",
        "    )\n",
        "    \n",
        "    print(response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 5: Exploring Temperature Parameter\n",
        "\n",
        "**Temperature** controls the randomness of the model's output:\n",
        "- **0.0**: Deterministic, always picks the most likely token\n",
        "- **0.7**: Balanced creativity and consistency (default)\n",
        "- **1.0+**: More creative and random\n",
        "\n",
        "For banking applications, we typically use lower temperatures (0.3-0.7) for consistency."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test different temperatures\n",
        "temperatures = [0.0, 0.5, 1.0, 1.5]\n",
        "question = \"Give me 3 tips for saving money.\"\n",
        "\n",
        "for temp in temperatures:\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"Temperature: {temp}\")\n",
        "    print(f\"{'='*80}\\n\")\n",
        "    \n",
        "    response = client.chat.completions.create(\n",
        "        model=GPT35_DEPLOYMENT,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a financial advisor.\"},\n",
        "            {\"role\": \"user\", \"content\": question}\n",
        "        ],\n",
        "        temperature=temp,\n",
        "        max_tokens=200\n",
        "    )\n",
        "    \n",
        "    print(response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 6: Building a Conversational Banking Assistant\n",
        "\n",
        "Let's build a simple chatbot that maintains conversation history."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class BankingAssistant:\n",
        "    def __init__(self, client, deployment_name):\n",
        "        self.client = client\n",
        "        self.deployment_name = deployment_name\n",
        "        self.conversation_history = [\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": \"\"\"You are a helpful banking assistant for Banco Nacional de Costa Rica (BNCR).\n",
        "                You help customers with:\n",
        "                - Account information\n",
        "                - Product recommendations\n",
        "                - General banking questions\n",
        "                - Transaction support\n",
        "                \n",
        "                Always be professional, friendly, and provide accurate information.\n",
        "                If you don't know something, admit it and suggest contacting customer service.\"\"\"\n",
        "            }\n",
        "        ]\n",
        "    \n",
        "    def chat(self, user_message):\n",
        "        # Add user message to history\n",
        "        self.conversation_history.append({\n",
        "            \"role\": \"user\",\n",
        "            \"content\": user_message\n",
        "        })\n",
        "        \n",
        "        # Get response from Azure OpenAI\n",
        "        response = self.client.chat.completions.create(\n",
        "            model=self.deployment_name,\n",
        "            messages=self.conversation_history,\n",
        "            temperature=0.7,\n",
        "            max_tokens=300\n",
        "        )\n",
        "        \n",
        "        # Extract assistant's response\n",
        "        assistant_message = response.choices[0].message.content\n",
        "        \n",
        "        # Add assistant's response to history\n",
        "        self.conversation_history.append({\n",
        "            \"role\": \"assistant\",\n",
        "            \"content\": assistant_message\n",
        "        })\n",
        "        \n",
        "        return assistant_message, response.usage\n",
        "    \n",
        "    def reset(self):\n",
        "        # Keep only the system message\n",
        "        self.conversation_history = [self.conversation_history[0]]\n",
        "        print(\"Conversation history reset.\")\n",
        "\n",
        "# Initialize the assistant\n",
        "assistant = BankingAssistant(client, GPT35_DEPLOYMENT)\n",
        "print(\"\u2713 Banking Assistant initialized\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test conversation with context\n",
        "print(\"User: Hello! I want to open a savings account.\\n\")\n",
        "response, usage = assistant.chat(\"Hello! I want to open a savings account.\")\n",
        "print(f\"Assistant: {response}\\n\")\n",
        "print(f\"Tokens used: {usage.total_tokens}\\n\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\\nUser: What documents do I need?\\n\")\n",
        "response, usage = assistant.chat(\"What documents do I need?\")\n",
        "print(f\"Assistant: {response}\\n\")\n",
        "print(f\"Tokens used: {usage.total_tokens}\\n\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\\nUser: What's the minimum deposit?\\n\")\n",
        "response, usage = assistant.chat(\"What's the minimum deposit?\")\n",
        "print(f\"Assistant: {response}\\n\")\n",
        "print(f\"Tokens used: {usage.total_tokens}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 7: Error Handling and Retry Logic\n",
        "\n",
        "Production applications need robust error handling. Let's implement retry logic for common errors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "from openai import RateLimitError, APIError, APIConnectionError\n",
        "\n",
        "def chat_with_retry(client, messages, deployment, max_retries=3):\n",
        "    \"\"\"\n",
        "    Make a chat completion request with exponential backoff retry\n",
        "    \"\"\"\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            response = client.chat.completions.create(\n",
        "                model=deployment,\n",
        "                messages=messages,\n",
        "                temperature=0.7,\n",
        "                max_tokens=200\n",
        "            )\n",
        "            return response\n",
        "        \n",
        "        except RateLimitError as e:\n",
        "            if attempt == max_retries - 1:\n",
        "                raise\n",
        "            wait_time = 2 ** attempt\n",
        "            print(f\"Rate limit exceeded. Retrying in {wait_time} seconds...\")\n",
        "            time.sleep(wait_time)\n",
        "        \n",
        "        except (APIError, APIConnectionError) as e:\n",
        "            if attempt == max_retries - 1:\n",
        "                raise\n",
        "            wait_time = 2 ** attempt\n",
        "            print(f\"API error occurred. Retrying in {wait_time} seconds...\")\n",
        "            time.sleep(wait_time)\n",
        "        \n",
        "        except Exception as e:\n",
        "            print(f\"Unexpected error: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "# Test the retry function\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a banking assistant.\"},\n",
        "    {\"role\": \"user\", \"content\": \"What is a credit score?\"}\n",
        "]\n",
        "\n",
        "try:\n",
        "    response = chat_with_retry(client, messages, GPT35_DEPLOYMENT)\n",
        "    print(\"Success!\")\n",
        "    print(response.choices[0].message.content)\n",
        "except Exception as e:\n",
        "    print(f\"Failed after retries: {str(e)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 8: Token Usage and Cost Optimization\n",
        "\n",
        "Understanding token usage is crucial for cost optimization.\n",
        "\n",
        "**Token Pricing (approximate):**\n",
        "- GPT-3.5 Turbo: $0.0015 per 1K input tokens, $0.002 per 1K output tokens\n",
        "- GPT-4: $0.03 per 1K input tokens, $0.06 per 1K output tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_cost(usage, model=\"gpt-35-turbo\"):\n",
        "    \"\"\"\n",
        "    Calculate the cost of an API call\n",
        "    \"\"\"\n",
        "    if \"gpt-4\" in model.lower():\n",
        "        input_cost = (usage.prompt_tokens / 1000) * 0.03\n",
        "        output_cost = (usage.completion_tokens / 1000) * 0.06\n",
        "    else:  # GPT-3.5\n",
        "        input_cost = (usage.prompt_tokens / 1000) * 0.0015\n",
        "        output_cost = (usage.completion_tokens / 1000) * 0.002\n",
        "    \n",
        "    total_cost = input_cost + output_cost\n",
        "    \n",
        "    return {\n",
        "        \"input_tokens\": usage.prompt_tokens,\n",
        "        \"output_tokens\": usage.completion_tokens,\n",
        "        \"total_tokens\": usage.total_tokens,\n",
        "        \"input_cost\": input_cost,\n",
        "        \"output_cost\": output_cost,\n",
        "        \"total_cost\": total_cost\n",
        "    }\n",
        "\n",
        "# Test cost calculation\n",
        "response = client.chat.completions.create(\n",
        "    model=GPT35_DEPLOYMENT,\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a banking assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": \"Explain the difference between a checking and savings account.\"}\n",
        "    ],\n",
        "    temperature=0.7,\n",
        "    max_tokens=300\n",
        ")\n",
        "\n",
        "cost_info = calculate_cost(response.usage, response.model)\n",
        "\n",
        "print(\"Response:\")\n",
        "print(response.choices[0].message.content)\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"\\nCost Analysis:\")\n",
        "print(f\"Input tokens: {cost_info['input_tokens']}\")\n",
        "print(f\"Output tokens: {cost_info['output_tokens']}\")\n",
        "print(f\"Total tokens: {cost_info['total_tokens']}\")\n",
        "print(f\"\\nInput cost: ${cost_info['input_cost']:.6f}\")\n",
        "print(f\"Output cost: ${cost_info['output_cost']:.6f}\")\n",
        "print(f\"Total cost: ${cost_info['total_cost']:.6f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 9: Comparing GPT-3.5 vs GPT-4\n",
        "\n",
        "Let's compare the responses and costs between GPT-3.5 and GPT-4."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Complex banking question\n",
        "complex_question = \"\"\"A customer has $50,000 to invest. They want low risk but better returns \n",
        "than a savings account. They might need access to the money in 2 years. \n",
        "What would you recommend and why?\"\"\"\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are an expert financial advisor at BNCR.\"},\n",
        "    {\"role\": \"user\", \"content\": complex_question}\n",
        "]\n",
        "\n",
        "# Test with GPT-3.5\n",
        "print(\"GPT-3.5 Turbo Response:\")\n",
        "print(\"=\"*80)\n",
        "response_35 = client.chat.completions.create(\n",
        "    model=GPT35_DEPLOYMENT,\n",
        "    messages=messages,\n",
        "    temperature=0.7,\n",
        "    max_tokens=400\n",
        ")\n",
        "print(response_35.choices[0].message.content)\n",
        "cost_35 = calculate_cost(response_35.usage, \"gpt-35-turbo\")\n",
        "print(f\"\\nCost: ${cost_35['total_cost']:.6f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
        "\n",
        "# Test with GPT-4 (if available)\n",
        "try:\n",
        "    print(\"GPT-4 Response:\")\n",
        "    print(\"=\"*80)\n",
        "    response_4 = client.chat.completions.create(\n",
        "        model=GPT4_DEPLOYMENT,\n",
        "        messages=messages,\n",
        "        temperature=0.7,\n",
        "        max_tokens=400\n",
        "    )\n",
        "    print(response_4.choices[0].message.content)\n",
        "    cost_4 = calculate_cost(response_4.usage, \"gpt-4\")\n",
        "    print(f\"\\nCost: ${cost_4['total_cost']:.6f}\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(f\"\\nCost Comparison:\")\n",
        "    print(f\"GPT-3.5: ${cost_35['total_cost']:.6f}\")\n",
        "    print(f\"GPT-4: ${cost_4['total_cost']:.6f}\")\n",
        "    print(f\"GPT-4 is {cost_4['total_cost']/cost_35['total_cost']:.1f}x more expensive\")\n",
        "except Exception as e:\n",
        "    print(f\"GPT-4 not available: {str(e)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud83c\udfaf Practical Exercise 1: Product Recommendation System\n",
        "\n",
        "Create a banking product recommendation system that:\n",
        "1. Asks the customer about their needs\n",
        "2. Recommends appropriate products\n",
        "3. Explains why each product is suitable\n",
        "\n",
        "Use the banking products dataset provided."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load banking products\n",
        "products_df = pd.read_csv(\"../../datasets/banking/banking_products.csv\")\n",
        "\n",
        "# Display available products\n",
        "print(\"Available Banking Products:\")\n",
        "print(products_df[['product_name', 'product_type', 'interest_rate']].to_string(index=False))\n",
        "\n",
        "# TODO: Create a system prompt that includes product information\n",
        "# TODO: Build a recommendation function\n",
        "# TODO: Test with different customer profiles\n",
        "\n",
        "# Your code here:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud83c\udfaf Practical Exercise 2: Multi-language Support\n",
        "\n",
        "Extend the banking assistant to support both Spanish and English.\n",
        "The assistant should:\n",
        "1. Detect the language of the user's message\n",
        "2. Respond in the same language\n",
        "3. Maintain conversation history in both languages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Create a multilingual banking assistant\n",
        "# TODO: Test with Spanish and English queries\n",
        "# TODO: Implement language detection\n",
        "\n",
        "# Your code here:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud83c\udfaf Practical Exercise 3: Cost Optimization Challenge\n",
        "\n",
        "You have a budget of $10 per day for API calls.\n",
        "\n",
        "Tasks:\n",
        "1. Calculate how many customer interactions you can handle\n",
        "2. Implement a token counter that warns when approaching limits\n",
        "3. Optimize the system prompt to reduce token usage\n",
        "4. Compare costs between GPT-3.5 and GPT-4 for your use case"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Implement daily budget tracker\n",
        "# TODO: Create cost optimization strategies\n",
        "# TODO: Compare model costs\n",
        "\n",
        "# Your code here:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary and Key Takeaways\n",
        "\n",
        "In this lab, you learned:\n",
        "\n",
        "1. **Azure OpenAI Basics**: How to initialize the client and make API calls\n",
        "2. **Message Roles**: System, user, and assistant roles and their purposes\n",
        "3. **Parameters**: Temperature, max_tokens, and their effects on output\n",
        "4. **Conversation Management**: Building chatbots with conversation history\n",
        "5. **Error Handling**: Implementing retry logic for production systems\n",
        "6. **Cost Optimization**: Understanding token usage and calculating costs\n",
        "7. **Model Comparison**: When to use GPT-3.5 vs GPT-4\n",
        "\n",
        "### Best Practices for Banking Applications:\n",
        "\n",
        "- Use **lower temperatures (0.3-0.7)** for consistent, reliable responses\n",
        "- Implement **comprehensive error handling** with retry logic\n",
        "- **Monitor token usage** to control costs\n",
        "- Use **clear system prompts** that define the assistant's role and limitations\n",
        "- **Never expose sensitive information** in prompts or logs\n",
        "- Always **validate and sanitize** user inputs\n",
        "- Implement **rate limiting** to prevent abuse\n",
        "\n",
        "### Next Steps:\n",
        "\n",
        "In the next lab, we'll explore advanced prompt engineering techniques to improve response quality and consistency.\n",
        "\n",
        "---\n",
        "\n",
        "**Questions or Issues?**  \n",
        "Contact: Manuela Larrea | manuela.larrea@idataglobal.com"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}