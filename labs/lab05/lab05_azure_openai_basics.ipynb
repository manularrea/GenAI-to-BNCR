{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 05: First Steps with Azure OpenAI\n",
    "\n",
    "**Course:** Generative AI for Banking Sector  \n",
    "**Institution:** Banco Nacional de Costa Rica (BNCR)  \n",
    "**Instructor:** Manuela Larrea  \n",
    "**Duration:** 3 hours\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this lab, you will be able to:\n",
    "\n",
    "1. Understand the Azure OpenAI Service architecture and authentication\n",
    "2. Make your first API calls to GPT-3.5 and GPT-4 models\n",
    "3. Explore different parameters (temperature, max_tokens, top_p)\n",
    "4. Build a simple banking assistant chatbot\n",
    "5. Handle errors and implement retry logic\n",
    "6. Understand token usage and cost optimization\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Azure Infrastructure for This Lab\n",
    "\n",
    "```\n",
    "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "â•‘                    LAB 05 - AZURE INFRASTRUCTURE                         â•‘\n",
    "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                        YOU (Jupyter Notebook)                            â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                                 â”‚\n",
    "                                 â”‚ HTTPS Request\n",
    "                                 â”‚ (API Key in Header)\n",
    "                                 â–¼\n",
    "                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "                    â”‚   Azure OpenAI Service     â”‚\n",
    "                    â”‚                            â”‚\n",
    "                    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚\n",
    "                    â”‚  â”‚  GPT-3.5 Turbo       â”‚  â”‚\n",
    "                    â”‚  â”‚  (60K TPM)           â”‚  â”‚\n",
    "                    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚\n",
    "                    â”‚                            â”‚\n",
    "                    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚\n",
    "                    â”‚  â”‚  GPT-4               â”‚  â”‚\n",
    "                    â”‚  â”‚  (10K TPM)           â”‚  â”‚\n",
    "                    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚\n",
    "                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "ğŸ“Š Resources Used:\n",
    "  â€¢ Azure OpenAI Service (East US 2)\n",
    "  â€¢ GPT-3.5 Turbo deployment\n",
    "  â€¢ GPT-4 deployment (optional)\n",
    "\n",
    "ğŸ’° Estimated Cost: ~$0.50 per lab session\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Environment Setup\n",
    "\n",
    "First, let's install the required packages and set up our environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install openai python-dotenv -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import AzureOpenAI\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "\n",
    "# Cargar environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Verify environment variables are loaded\n",
    "print(\"âœ“ Environment variables loaded\")\n",
    "print(f\"âœ“ Azure OpenAI Endpoint: {os.getenv('AZURE_OPENAI_ENDPOINT')[:30]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Initialize Azure OpenAI Client\n",
    "\n",
    "The Azure OpenAI client requires three key pieces of information:\n",
    "1. **API Key**: Your authentication credential\n",
    "2. **Endpoint**: Your Azure OpenAI resource URL\n",
    "3. **API Version**: The version of the API to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar Azure OpenAI client\n",
    "client = AzureOpenAI(\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "    api_version=\"2024-02-15-preview\",\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    ")\n",
    "\n",
    "# Deployment names\n",
    "GPT35_DEPLOYMENT = os.getenv(\"AZURE_OPENAI_DEPLOYMENT_GPT35\", \"gpt-35-turbo\")\n",
    "GPT4_DEPLOYMENT = os.getenv(\"AZURE_OPENAI_DEPLOYMENT_GPT4\", \"gpt-4\")\n",
    "\n",
    "print(\"âœ“ Azure OpenAI client initialized successfully\")\n",
    "print(f\"âœ“ GPT-3.5 Deployment: {GPT35_DEPLOYMENT}\")\n",
    "print(f\"âœ“ GPT-4 Deployment: {GPT4_DEPLOYMENT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Your First API Call\n",
    "\n",
    "Let's make our first call to Azure OpenAI! We'll create a simple banking assistant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CompletaciÃ³n de chat simple\n",
    "response = client.chat.completions.create(\n",
    "    model=GPT35_DEPLOYMENT,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"Eres un asistente bancario profesional del BNCR.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Â¿QuÃ© es una cuenta de ahorros?\"}\n",
    "    ],\n",
    "    temperature=0.7,\n",
    "    max_tokens=150\n",
    ")\n",
    "\n",
    "print(\"Respuesta del Asistente:\")\n",
    "print(response.choices[0].message.content)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"Tokens usados: {response.usage.total_tokens}\")\n",
    "print(f\"Modelo: {response.model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Understanding Message Roles\n",
    "\n",
    "Azure OpenAI uses three message roles:\n",
    "\n",
    "1. **System**: Sets the behavior and context for the assistant\n",
    "2. **User**: The user's input or question\n",
    "3. **Assistant**: The model's previous responses (for conversation history)\n",
    "\n",
    "Let's see how different system prompts affect the responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probar different system prompts\n",
    "system_prompts = [\n",
    "    \"You are a formal banking advisor.\",\n",
    "    \"You are a friendly banking assistant who explains things simply.\",\n",
    "    \"You are a technical banking expert who provides detailed explanations.\"\n",
    "]\n",
    "\n",
    "user_question = \"What is compound interest?\"\n",
    "\n",
    "for i, system_prompt in enumerate(system_prompts, 1):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Test {i}: {system_prompt}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=GPT35_DEPLOYMENT,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_question}\n",
    "        ],\n",
    "        temperature=0.7,\n",
    "        max_tokens=150\n",
    "    )\n",
    "    \n",
    "    print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Exploring Temperature Parameter\n",
    "\n",
    "**Temperature** controls the randomness of the model's output:\n",
    "- **0.0**: Deterministic, always picks the most likely token\n",
    "- **0.7**: Balanced creativity and consistency (default)\n",
    "- **1.0+**: More creative and random\n",
    "\n",
    "For banking applications, we typically use lower temperatures (0.3-0.7) for consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probar different temperatures\n",
    "temperatures = [0.0, 0.5, 1.0, 1.5]\n",
    "question = \"Give me 3 tips for saving money.\"\n",
    "\n",
    "for temp in temperatures:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Temperatura: {temp}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=GPT35_DEPLOYMENT,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a financial advisor.\"},\n",
    "            {\"role\": \"user\", \"content\": question}\n",
    "        ],\n",
    "        temperature=temp,\n",
    "        max_tokens=200\n",
    "    )\n",
    "    \n",
    "    print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Building a Conversational Banking Assistant\n",
    "\n",
    "Let's build a simple chatbot that maintains conversation history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BankingAssistant:\n",
    "    def __init__(self, client, deployment_name):\n",
    "        self.client = client\n",
    "        self.deployment_name = deployment_name\n",
    "        self.conversation_history = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"\"\"Eres un asistente bancario profesional del Banco Nacional de Costa Rica (BNCR).\n",
    "                Ayudas a los clientes con:\n",
    "                - InformaciÃ³n de cuentas\n",
    "                - Recomendaciones de productos\n",
    "                - Preguntas generales de banca\n",
    "                - Transaction support\n",
    "                \n",
    "                Always be professional, friendly, and provide accurate information.\n",
    "                If you don't know something, admit it and suggest contacting customer service.\"\"\"\n",
    "            }\n",
    "        ]\n",
    "    \n",
    "    def chat(self, user_message):\n",
    "        # Add user message to history\n",
    "        self.conversation_history.append({\n",
    "            \"role\": \"user\",\n",
    "            \"content\": user_message\n",
    "        })\n",
    "        \n",
    "        # Get response from Azure OpenAI\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=self.deployment_name,\n",
    "            messages=self.conversation_history,\n",
    "            temperature=0.7,\n",
    "            max_tokens=300\n",
    "        )\n",
    "        \n",
    "        # Extract assistant's response\n",
    "        assistant_message = response.choices[0].message.content\n",
    "        \n",
    "        # Add assistant's response to history\n",
    "        self.conversation_history.append({\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": assistant_message\n",
    "        })\n",
    "        \n",
    "        return assistant_message, response.usage\n",
    "    \n",
    "    def reset(self):\n",
    "        # Keep only the system message\n",
    "        self.conversation_history = [self.conversation_history[0]]\n",
    "        print(\"Conversation history reset.\")\n",
    "\n",
    "# Inicializar the assistant\n",
    "assistant = BankingAssistant(client, GPT35_DEPLOYMENT)\n",
    "print(\"âœ“ Banking Assistant initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probar conversation with context\n",
    "print(\"User: Hello! I want to open a savings account.\\n\")\n",
    "response, usage = assistant.chat(\"Hello! I want to open a savings account.\")\n",
    "print(f\"Assistant: {response}\\n\")\n",
    "print(f\"Tokens usados: {usage.total_tokens}\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nUser: Â¿QuÃ© documentos necesito?\\n\")\n",
    "response, usage = assistant.chat(\"Â¿QuÃ© documentos necesito?\")\n",
    "print(f\"Assistant: {response}\\n\")\n",
    "print(f\"Tokens usados: {usage.total_tokens}\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nUser: What's the minimum deposit?\\n\")\n",
    "response, usage = assistant.chat(\"What's the minimum deposit?\")\n",
    "print(f\"Assistant: {response}\\n\")\n",
    "print(f\"Tokens usados: {usage.total_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Error Handling and Retry Logic\n",
    "\n",
    "Production applications need robust error handling. Let's implement retry logic for common errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from openai import RateLimitError, APIError, APIConnectionError\n",
    "\n",
    "def chat_with_retry(client, messages, deployment, max_retries=3):\n",
    "    \"\"\"\n",
    "    Hacer una solicitud de completaciÃ³n con reintento exponencial\n",
    "    \"\"\"\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=deployment,\n",
    "                messages=messages,\n",
    "                temperature=0.7,\n",
    "                max_tokens=200\n",
    "            )\n",
    "            return response\n",
    "        \n",
    "        except RateLimitError as e:\n",
    "            if attempt == max_retries - 1:\n",
    "                raise\n",
    "            wait_time = 2 ** attempt\n",
    "            print(f\"Rate limit exceeded. Retrying in {wait_time} seconds...\")\n",
    "            time.sleep(wait_time)\n",
    "        \n",
    "        except (APIError, APIConnectionError) as e:\n",
    "            if attempt == max_retries - 1:\n",
    "                raise\n",
    "            wait_time = 2 ** attempt\n",
    "            print(f\"API error occurred. Retrying in {wait_time} seconds...\")\n",
    "            time.sleep(wait_time)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Unexpected error: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "# Probar the retry function\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a banking assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is a credit score?\"}\n",
    "]\n",
    "\n",
    "try:\n",
    "    response = chat_with_retry(client, messages, GPT35_DEPLOYMENT)\n",
    "    print(\"Success!\")\n",
    "    print(response.choices[0].message.content)\n",
    "except Exception as e:\n",
    "    print(f\"Failed after retries: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Token Usage and Cost Optimization\n",
    "\n",
    "Understanding token usage is crucial for cost optimization.\n",
    "\n",
    "**Token Pricing (approximate):**\n",
    "- GPT-3.5 Turbo: $0.0015 per 1K input tokens, $0.002 per 1K output tokens\n",
    "- GPT-4: $0.03 per 1K input tokens, $0.06 per 1K output tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cost(usage, model=\"gpt-35-turbo\"):\n",
    "    \"\"\"\n",
    "    Calculate the cost of an API call\n",
    "    \"\"\"\n",
    "    if \"gpt-4\" in model.lower():\n",
    "        input_cost = (usage.prompt_tokens / 1000) * 0.03\n",
    "        output_cost = (usage.completion_tokens / 1000) * 0.06\n",
    "    else:  # GPT-3.5\n",
    "        input_cost = (usage.prompt_tokens / 1000) * 0.0015\n",
    "        output_cost = (usage.completion_tokens / 1000) * 0.002\n",
    "    \n",
    "    total_cost = input_cost + output_cost\n",
    "    \n",
    "    return {\n",
    "        \"input_tokens\": usage.prompt_tokens,\n",
    "        \"output_tokens\": usage.completion_tokens,\n",
    "        \"total_tokens\": usage.total_tokens,\n",
    "        \"input_cost\": input_cost,\n",
    "        \"output_cost\": output_cost,\n",
    "        \"total_cost\": total_cost\n",
    "    }\n",
    "\n",
    "# Probar cost calculation\n",
    "response = client.chat.completions.create(\n",
    "    model=GPT35_DEPLOYMENT,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a banking assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Explain the difference between a checking and savings account.\"}\n",
    "    ],\n",
    "    temperature=0.7,\n",
    "    max_tokens=300\n",
    ")\n",
    "\n",
    "cost_info = calculate_cost(response.usage, response.model)\n",
    "\n",
    "print(\"Respuesta:\")\n",
    "print(response.choices[0].message.content)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\\nCost Analysis:\")\n",
    "print(f\"Input tokens: {cost_info['input_tokens']}\")\n",
    "print(f\"Output tokens: {cost_info['output_tokens']}\")\n",
    "print(f\"Total tokens: {cost_info['total_tokens']}\")\n",
    "print(f\"\\nInput cost: ${cost_info['input_cost']:.6f}\")\n",
    "print(f\"Output cost: ${cost_info['output_cost']:.6f}\")\n",
    "print(f\"Total cost: ${cost_info['total_cost']:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 9: Comparing GPT-3.5 vs GPT-4\n",
    "\n",
    "Let's compare the responses and costs between GPT-3.5 and GPT-4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complex banking question\n",
    "complex_question = \"\"\"A customer has $50,000 to invest. They want low risk but better returns \n",
    "than a savings account. They might need access to the money in 2 years. \n",
    "What would you recommend and why?\"\"\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are an expert financial advisor at BNCR.\"},\n",
    "    {\"role\": \"user\", \"content\": complex_question}\n",
    "]\n",
    "\n",
    "# Probar with GPT-3.5\n",
    "print(\"GPT-3.5 Turbo Respuesta:\")\n",
    "print(\"=\"*80)\n",
    "response_35 = client.chat.completions.create(\n",
    "    model=GPT35_DEPLOYMENT,\n",
    "    messages=messages,\n",
    "    temperature=0.7,\n",
    "    max_tokens=400\n",
    ")\n",
    "print(response_35.choices[0].message.content)\n",
    "cost_35 = calculate_cost(response_35.usage, \"gpt-35-turbo\")\n",
    "print(f\"\\nCost: ${cost_35['total_cost']:.6f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# Probar with GPT-4 (if available)\n",
    "try:\n",
    "    print(\"GPT-4 Respuesta:\")\n",
    "    print(\"=\"*80)\n",
    "    response_4 = client.chat.completions.create(\n",
    "        model=GPT4_DEPLOYMENT,\n",
    "        messages=messages,\n",
    "        temperature=0.7,\n",
    "        max_tokens=400\n",
    "    )\n",
    "    print(response_4.choices[0].message.content)\n",
    "    cost_4 = calculate_cost(response_4.usage, \"gpt-4\")\n",
    "    print(f\"\\nCost: ${cost_4['total_cost']:.6f}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"\\nCost Comparison:\")\n",
    "    print(f\"GPT-3.5: ${cost_35['total_cost']:.6f}\")\n",
    "    print(f\"GPT-4: ${cost_4['total_cost']:.6f}\")\n",
    "    print(f\"GPT-4 is {cost_4['total_cost']/cost_35['total_cost']:.1f}x more expensive\")\n",
    "except Exception as e:\n",
    "    print(f\"GPT-4 not available: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ Practical Exercise 1: Product Recommendation System\n",
    "\n",
    "Create a banking product recommendation system that:\n",
    "1. Asks the customer about their needs\n",
    "2. Recommends appropriate products\n",
    "3. Explains why each product is suitable\n",
    "\n",
    "Use the banking products dataset provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Cargar banking products\n",
    "products_df = pd.read_csv(\"../../datasets/banking/banking_products.csv\")\n",
    "\n",
    "# Mostrar available products\n",
    "print(\"Productos Bancarios Disponibles:\")\n",
    "print(products_df[['product_name', 'product_type', 'interest_rate']].to_string(index=False))\n",
    "\n",
    "# TODO: Crear un sistema prompt that includes product information\n",
    "# TODO: Build a recommendation function\n",
    "# TODO: Test with different customer profiles\n",
    "\n",
    "# Tu cÃ³digo aquÃ­:\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ Practical Exercise 2: Multi-language Support\n",
    "\n",
    "Extend the banking assistant to support both Spanish and English.\n",
    "The assistant should:\n",
    "1. Detect the language of the user's message\n",
    "2. Respond in the same language\n",
    "3. Maintain conversation history in both languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a multilingual banking assistant\n",
    "# TODO: Probar con consultas en espaÃ±ol e inglÃ©s\n",
    "# TODO: Implementar detecciÃ³n de idioma\n",
    "\n",
    "# Tu cÃ³digo aquÃ­:\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ Practical Exercise 3: Cost Optimization Challenge\n",
    "\n",
    "You have a budget of $10 per day for API calls.\n",
    "\n",
    "Tasks:\n",
    "1. Calculate how many customer interactions you can handle\n",
    "2. Implement a token counter that warns when approaching limits\n",
    "3. Optimize the system prompt to reduce token usage\n",
    "4. Compare costs between GPT-3.5 and GPT-4 for your use case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implementar rastreador de presupuesto diario\n",
    "# TODO: Crear estrategias de optimizaciÃ³n de costos\n",
    "# TODO: Comparar costos de modelos\n",
    "\n",
    "# Tu cÃ³digo aquÃ­:\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Key Takeaways\n",
    "\n",
    "In this lab, you learned:\n",
    "\n",
    "1. **Azure OpenAI Basics**: How to initialize the client and make API calls\n",
    "2. **Message Roles**: System, user, and assistant roles and their purposes\n",
    "3. **Parameters**: Temperature, max_tokens, and their effects on output\n",
    "4. **Conversation Management**: Building chatbots with conversation history\n",
    "5. **Error Handling**: Implementing retry logic for production systems\n",
    "6. **Cost Optimization**: Understanding token usage and calculating costs\n",
    "7. **Model Comparison**: When to use GPT-3.5 vs GPT-4\n",
    "\n",
    "### Best Practices for Banking Applications:\n",
    "\n",
    "- Use **lower temperatures (0.3-0.7)** for consistent, reliable responses\n",
    "- Implement **comprehensive error handling** with retry logic\n",
    "- **Monitor token usage** to control costs\n",
    "- Use **clear system prompts** that define the assistant's role and limitations\n",
    "- **Never expose sensitive information** in prompts or logs\n",
    "- Always **validate and sanitize** user inputs\n",
    "- Implement **rate limiting** to prevent abuse\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "In the next lab, we'll explore advanced prompt engineering techniques to improve response quality and consistency.\n",
    "\n",
    "---\n",
    "\n",
    "**Questions or Issues?**  \n",
    "Contact: Manuela Larrea | manuela.larrea@idataglobal.com"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}